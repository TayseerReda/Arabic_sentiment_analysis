{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhHOcqz9bYMs",
        "outputId": "53200b80-1038-4e6e-f86b-e08192f8194f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.9.0-py2.py3-none-any.whl (397 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.5/397.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.9.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore',category=FutureWarning)\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import ISRIStemmer #arabic stemming\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from scipy.special import softmax\n",
        "#!pip  install -U farasapy #limitization arabic $ remove il tashkeel\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "!pip install emoji\n",
        "import emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ETQhhTocTw2",
        "outputId": "93afe0a4-55e6-4590-ff75-7aba8c35bab6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي', 'الذي', 'الذين', 'اللاتي', 'اللائي', 'اللتان', 'اللتيا', 'اللتين', 'اللذان', 'اللذين', 'اللواتي', 'إلى', 'إليك', 'إليكم', 'إليكما', 'إليكن', 'أم', 'أما', 'أما', 'إما', 'أن', 'إن', 'إنا', 'أنا', 'أنت', 'أنتم', 'أنتما', 'أنتن', 'إنما', 'إنه', 'أنى', 'أنى', 'آه', 'آها', 'أو', 'أولاء', 'أولئك', 'أوه', 'آي', 'أي', 'أيها', 'إي', 'أين', 'أين', 'أينما', 'إيه', 'بخ', 'بس', 'بعد', 'بعض', 'بك', 'بكم', 'بكم', 'بكما', 'بكن', 'بل', 'بلى', 'بما', 'بماذا', 'بمن', 'بنا', 'به', 'بها', 'بهم', 'بهما', 'بهن', 'بي', 'بين', 'بيد', 'تلك', 'تلكم', 'تلكما', 'ته', 'تي', 'تين', 'تينك', 'ثم', 'ثمة', 'حاشا', 'حبذا', 'حتى', 'حيث', 'حيثما', 'حين', 'خلا', 'دون', 'ذا', 'ذات', 'ذاك', 'ذان', 'ذانك', 'ذلك', 'ذلكم', 'ذلكما', 'ذلكن', 'ذه', 'ذو', 'ذوا', 'ذواتا', 'ذواتي', 'ذي', 'ذين', 'ذينك', 'ريث', 'سوف', 'سوى', 'شتان', 'عدا', 'عسى', 'عل', 'على', 'عليك', 'عليه', 'عما', 'عن', 'عند', 'غير', 'فإذا', 'فإن', 'فلا', 'فمن', 'في', 'فيم', 'فيما', 'فيه', 'فيها', 'قد', 'كأن', 'كأنما', 'كأي', 'كأين', 'كذا', 'كذلك', 'كل', 'كلا', 'كلاهما', 'كلتا', 'كلما', 'كليكما', 'كليهما', 'كم', 'كم', 'كما', 'كي', 'كيت', 'كيف', 'كيفما', 'لا', 'لاسيما', 'لدى', 'لست', 'لستم', 'لستما', 'لستن', 'لسن', 'لسنا', 'لعل', 'لك', 'لكم', 'لكما', 'لكن', 'لكنما', 'لكي', 'لكيلا', 'لم', 'لما', 'لن', 'لنا', 'له', 'لها', 'لهم', 'لهما', 'لهن', 'لو', 'لولا', 'لوما', 'لي', 'لئن', 'ليت', 'ليس', 'ليسا', 'ليست', 'ليستا', 'ليسوا', 'ما', 'ماذا', 'متى', 'مذ', 'مع', 'مما', 'ممن', 'من', 'منه', 'منها', 'منذ', 'مه', 'مهما', 'نحن', 'نحو', 'نعم', 'ها', 'هاتان', 'هاته', 'هاتي', 'هاتين', 'هاك', 'هاهنا', 'هذا', 'هذان', 'هذه', 'هذي', 'هذين', 'هكذا', 'هل', 'هلا', 'هم', 'هما', 'هن', 'هنا', 'هناك', 'هنالك', 'هو', 'هؤلاء', 'هي', 'هيا', 'هيت', 'هيهات', 'والذي', 'والذين', 'وإذ', 'وإذا', 'وإن', 'ولا', 'ولكن', 'ولو', 'وما', 'ومن', 'وهو', 'يا', 'أبٌ', 'أخٌ', 'حمٌ', 'فو', 'أنتِ', 'يناير', 'فبراير', 'مارس', 'أبريل', 'مايو', 'يونيو', 'يوليو', 'أغسطس', 'سبتمبر', 'أكتوبر', 'نوفمبر', 'ديسمبر', 'جانفي', 'فيفري', 'مارس', 'أفريل', 'ماي', 'جوان', 'جويلية', 'أوت', 'كانون', 'شباط', 'آذار', 'نيسان', 'أيار', 'حزيران', 'تموز', 'آب', 'أيلول', 'تشرين', 'دولار', 'دينار', 'ريال', 'درهم', 'ليرة', 'جنيه', 'قرش', 'مليم', 'فلس', 'هللة', 'سنتيم', 'يورو', 'ين', 'يوان', 'شيكل', 'واحد', 'اثنان', 'ثلاثة', 'أربعة', 'خمسة', 'ستة', 'سبعة', 'ثمانية', 'تسعة', 'عشرة', 'أحد', 'اثنا', 'اثني', 'إحدى', 'ثلاث', 'أربع', 'خمس', 'ست', 'سبع', 'ثماني', 'تسع', 'عشر', 'ثمان', 'سبت', 'أحد', 'اثنين', 'ثلاثاء', 'أربعاء', 'خميس', 'جمعة', 'أول', 'ثان', 'ثاني', 'ثالث', 'رابع', 'خامس', 'سادس', 'سابع', 'ثامن', 'تاسع', 'عاشر', 'حادي', 'أ', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'ء', 'ى', 'آ', 'ؤ', 'ئ', 'أ', 'ة', 'ألف', 'باء', 'تاء', 'ثاء', 'جيم', 'حاء', 'خاء', 'دال', 'ذال', 'راء', 'زاي', 'سين', 'شين', 'صاد', 'ضاد', 'طاء', 'ظاء', 'عين', 'غين', 'فاء', 'قاف', 'كاف', 'لام', 'ميم', 'نون', 'هاء', 'واو', 'ياء', 'همزة', 'ي', 'نا', 'ك', 'كن', 'ه', 'إياه', 'إياها', 'إياهما', 'إياهم', 'إياهن', 'إياك', 'إياكما', 'إياكم', 'إياك', 'إياكن', 'إياي', 'إيانا', 'أولالك', 'تانِ', 'تانِك', 'تِه', 'تِي', 'تَيْنِ', 'ثمّ', 'ثمّة', 'ذانِ', 'ذِه', 'ذِي', 'ذَيْنِ', 'هَؤلاء', 'هَاتانِ', 'هَاتِه', 'هَاتِي', 'هَاتَيْنِ', 'هَذا', 'هَذانِ', 'هَذِه', 'هَذِي', 'هَذَيْنِ', 'الألى', 'الألاء', 'أل', 'أنّى', 'أيّ', 'ّأيّان', 'أنّى', 'أيّ', 'ّأيّان', 'ذيت', 'كأيّ', 'كأيّن', 'بضع', 'فلان', 'وا', 'آمينَ', 'آهِ', 'آهٍ', 'آهاً', 'أُفٍّ', 'أُفٍّ', 'أفٍّ', 'أمامك', 'أمامكَ', 'أوّهْ', 'إلَيْكَ', 'إلَيْكَ', 'إليكَ', 'إليكنّ', 'إيهٍ', 'بخٍ', 'بسّ', 'بَسْ', 'بطآن', 'بَلْهَ', 'حاي', 'حَذارِ', 'حيَّ', 'حيَّ', 'دونك', 'رويدك', 'سرعان', 'شتانَ', 'شَتَّانَ', 'صهْ', 'صهٍ', 'طاق', 'طَق', 'عَدَسْ', 'كِخ', 'مكانَك', 'مكانَك', 'مكانَك', 'مكانكم', 'مكانكما', 'مكانكنّ', 'نَخْ', 'هاكَ', 'هَجْ', 'هلم', 'هيّا', 'هَيْهات', 'وا', 'واهاً', 'وراءَك', 'وُشْكَانَ', 'وَيْ', 'يفعلان', 'تفعلان', 'يفعلون', 'تفعلون', 'تفعلين', 'اتخذ', 'ألفى', 'تخذ', 'ترك', 'تعلَّم', 'جعل', 'حجا', 'حبيب', 'خال', 'حسب', 'خال', 'درى', 'رأى', 'زعم', 'صبر', 'ظنَّ', 'عدَّ', 'علم', 'غادر', 'ذهب', 'وجد', 'ورد', 'وهب', 'أسكن', 'أطعم', 'أعطى', 'رزق', 'زود', 'سقى', 'كسا', 'أخبر', 'أرى', 'أعلم', 'أنبأ', 'حدَث', 'خبَّر', 'نبَّا', 'أفعل به', 'ما أفعله', 'بئس', 'ساء', 'طالما', 'قلما', 'لات', 'لكنَّ', 'ءَ', 'أجل', 'إذاً', 'أمّا', 'إمّا', 'إنَّ', 'أنًّ', 'أى', 'إى', 'أيا', 'ب', 'ثمَّ', 'جلل', 'جير', 'رُبَّ', 'س', 'علًّ', 'ف', 'كأنّ', 'كلَّا', 'كى', 'ل', 'لات', 'لعلَّ', 'لكنَّ', 'لكنَّ', 'م', 'نَّ', 'هلّا', 'وا', 'أل', 'إلّا', 'ت', 'ك', 'لمّا', 'ن', 'ه', 'و', 'ا', 'ي', 'تجاه', 'تلقاء', 'جميع', 'حسب', 'سبحان', 'شبه', 'لعمر', 'مثل', 'معاذ', 'أبو', 'أخو', 'حمو', 'فو', 'مئة', 'مئتان', 'ثلاثمئة', 'أربعمئة', 'خمسمئة', 'ستمئة', 'سبعمئة', 'ثمنمئة', 'تسعمئة', 'مائة', 'ثلاثمائة', 'أربعمائة', 'خمسمائة', 'ستمائة', 'سبعمائة', 'ثمانمئة', 'تسعمائة', 'عشرون', 'ثلاثون', 'اربعون', 'خمسون', 'ستون', 'سبعون', 'ثمانون', 'تسعون', 'عشرين', 'ثلاثين', 'اربعين', 'خمسين', 'ستين', 'سبعين', 'ثمانين', 'تسعين', 'بضع', 'نيف', 'أجمع', 'جميع', 'عامة', 'عين', 'نفس', 'لا سيما', 'أصلا', 'أهلا', 'أيضا', 'بؤسا', 'بعدا', 'بغتة', 'تعسا', 'حقا', 'حمدا', 'خلافا', 'خاصة', 'دواليك', 'سحقا', 'سرا', 'سمعا', 'صبرا', 'صدقا', 'صراحة', 'طرا', 'عجبا', 'عيانا', 'غالبا', 'فرادى', 'فضلا', 'قاطبة', 'كثيرا', 'لبيك', 'معاذ', 'أبدا', 'إزاء', 'أصلا', 'الآن', 'أمد', 'أمس', 'آنفا', 'آناء', 'أنّى', 'أول', 'أيّان', 'تارة', 'ثمّ', 'ثمّة', 'حقا', 'صباح', 'مساء', 'ضحوة', 'عوض', 'غدا', 'غداة', 'قطّ', 'كلّما', 'لدن', 'لمّا', 'مرّة', 'قبل', 'خلف', 'أمام', 'فوق', 'تحت', 'يمين', 'شمال', 'ارتدّ', 'استحال', 'أصبح', 'أضحى', 'آض', 'أمسى', 'انقلب', 'بات', 'تبدّل', 'تحوّل', 'حار', 'رجع', 'راح', 'صار', 'ظلّ', 'عاد', 'غدا', 'كان', 'ما انفك', 'ما برح', 'مادام', 'مازال', 'مافتئ', 'ابتدأ', 'أخذ', 'اخلولق', 'أقبل', 'انبرى', 'أنشأ', 'أوشك', 'جعل', 'حرى', 'شرع', 'طفق', 'علق', 'قام', 'كرب', 'كاد', 'هبّ']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('arabic')\n",
        "print(stopwords )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MoaEdh0uHHI"
      },
      "outputs": [],
      "source": [
        "\n",
        "def preprocessing(data):\n",
        "\n",
        "    text_no_emoji = emoji.demojize(data)#remove emojies\n",
        "    data = re.sub(r':[a-zA-Z_]+:', ' ', text_no_emoji)\n",
        "\n",
        "\n",
        "    review = re.sub(\"[ًٌٍَُِّْـ]\", \"\", data)#remove tashkeel\n",
        "    review = re.sub('[A-Za-z0-9]',' ',review)#remove english and numbers\n",
        "\n",
        "    ##############################################\n",
        "\n",
        "    # Normalize elongated words\n",
        "    review = re.sub(r'(\\w)\\1{2,}', r'\\1', review) #اوووو-> او\n",
        "\n",
        "     # Handling Negations\n",
        "    review = re.sub(r\"\\b(?:لا|لم|لن|ما|من|ألا|لست)\\b\", \"not\", review)\n",
        "\n",
        "\n",
        "    ##############################################\n",
        "\n",
        "    #replace every confused letter with general one\n",
        "    review = re.sub(\"[إأآا]\", \"ا\", review)\n",
        "    review = re.sub(\"ى\", \"ي\", review)\n",
        "    review = re.sub(\"ؤ\", \"ء\",review)\n",
        "    review = re.sub(\"ئ\", \"ء\",review)\n",
        "    review = re.sub(\"ة\", \"ه\", review)\n",
        "    review = re.sub(\"گ\", \"ك\", review)\n",
        "\n",
        "    punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ''' + string.punctuation #remove punctuations\n",
        "    translator = str.maketrans('', '', punctuations)\n",
        "    review = review.translate(translator)\n",
        "\n",
        "\n",
        "    #replace spaces and tabs and so on\n",
        "    review = re.sub(r\"\\n+\", ' ', review)\n",
        "    review = re.sub(r\"\\t+\", ' ', review)\n",
        "    review= re.sub(r\"\\r+\", ' ',review)\n",
        "    review = re.sub(r\"\\s+\", ' ', review)\n",
        "\n",
        "    review=nltk.word_tokenize(review)#tokenization\n",
        "\n",
        "    review=[w for w in review if not w in stopwords ]#remove stop word\n",
        "    isri_stemmer = ISRIStemmer() #stemming for arabic\n",
        "    review=[ISRIStemmer().suf32(w)  for w in review]\n",
        "    review = ' '.join(review)\n",
        "    return review\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUyLfAG95iW9"
      },
      "outputs": [],
      "source": [
        "data = pd.read_excel('/content/train.xlsx')\n",
        "xtrain= data.review_description.apply(preprocessing)#applies the preprocessing function to each element in the 'review_description' column of the DataFrame\n",
        "ytrain=data['rating']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HuGemuYuRXW",
        "outputId": "8d973d5d-a476-4aa4-fc04-a698e3611cfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "شركه زباله سواق بتبرشم مفيش حتي رقم للشكاوي السواق يسيبك يمشي ميرضاش يقف يقولك اعملك ايه اتصل بالشركه قول مرضيش يقفلي رغم اني حاجز not قبل بيوم\n"
          ]
        }
      ],
      "source": [
        "print(xtrain[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVN4FJUZtyDv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "035bc835-8eb7-4354-def5-56a59470035d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30374 30374\n"
          ]
        }
      ],
      "source": [
        "#remove spaces and drop every empty row\n",
        "j=0\n",
        "\n",
        "for text in xtrain:\n",
        "  if text.strip():\n",
        "    xtrain[j]=text.strip()\n",
        "  else :\n",
        "\n",
        "    xtrain = xtrain.drop(j)\n",
        "    ytrain=ytrain.drop(j)\n",
        "  j+=1\n",
        "\n",
        "\n",
        "#for text, label in zip(xtrain, ytrain):\n",
        "    #if text.strip():  # Check if the text is not an empty string after stripping whitespace\n",
        "        #X_train.append(text)\n",
        "       # Y_train.append(label)\n",
        "#print(len(c),len(a))\n",
        "#print(c)\n",
        "#print(len( X_train),len(Y_train))\n",
        "\n",
        "print(len(xtrain),len(ytrain))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fJoh7-TbITx"
      },
      "outputs": [],
      "source": [
        "  tokenizer = Tokenizer() #toknize and padd\n",
        "  tokenizer.fit_on_texts(xtrain)\n",
        "  vocab_size = len(tokenizer.word_index) + 1\n",
        "  sequences = tokenizer.texts_to_sequences(xtrain)\n",
        "  max_length = max(len(seq) for seq in sequences)\n",
        "  X_train = pad_sequences(sequences, maxlen=max_length, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3IcKOVqhZGr"
      },
      "outputs": [],
      "source": [
        "# Convert labels to one-hot encoding\n",
        "\n",
        "y_train = np.zeros((len(ytrain),3))\n",
        "for i, label in enumerate(ytrain):\n",
        "    if label == 1:\n",
        "        y_train[i, 0] = 1  # Positive\n",
        "    elif label == -1:\n",
        "        y_train[i, 1] = 1  # Negative\n",
        "    else:\n",
        "        y_train[i, 2] = 1  # Neutral\n",
        "\n",
        "#print(y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsgZA05F4jQG",
        "outputId": "971d526f-8f7c-4dd8-c04d-de23094ad5b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31024 327\n"
          ]
        }
      ],
      "source": [
        "print(vocab_size,max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRGd3w5pHwDa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73388fed-5063-46f0-a324-735aec01c264"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1. 0.] [1. 0. 0.] [0. 0. 1.]\n"
          ]
        }
      ],
      "source": [
        "print( y_train[0],y_train[6],y_train[5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DMTfbMbqKhZ"
      },
      "outputs": [],
      "source": [
        "X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train,test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_hTYeQmosgE",
        "outputId": "0450c57d-09f9-451c-9e5b-a1d1fe7f555e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "760/760 [==============================] - 540s 699ms/step - loss: 0.5051 - accuracy: 0.8086 - val_loss: 0.4731 - val_accuracy: 0.8272\n",
            "Epoch 2/2\n",
            "760/760 [==============================] - 522s 687ms/step - loss: 0.3343 - accuracy: 0.8852 - val_loss: 0.5123 - val_accuracy: 0.8056\n",
            "760/760 [==============================] - 133s 174ms/step - loss: 0.2351 - accuracy: 0.9173\n",
            "Training Accuracy: 0.9173\n",
            "190/190 [==============================] - 34s 180ms/step - loss: 0.5123 - accuracy: 0.8056\n",
            "Testing Accuracy:  0.8056\n"
          ]
        }
      ],
      "source": [
        "#bidirectional lstm\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length))\n",
        "model.add(Bidirectional(LSTM(100)))\n",
        "#model.add(Dense(100, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train_final, y_train_final, epochs=2, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "loss, accuracy = model.evaluate(X_train_final, y_train_final, verbose=True)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss_val, accuracy_val = model.evaluate(X_val, y_val, verbose=True)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy_val))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiEuOKZp5y-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7416821e-4ffb-4a5f-8e7b-c313f495b6b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "760/760 [==============================] - 200s 263ms/step - loss: 0.5048 - accuracy: 0.8105 - val_loss: 0.4795 - val_accuracy: 0.8250\n",
            "Epoch 2/2\n",
            "760/760 [==============================] - 180s 236ms/step - loss: 0.2631 - accuracy: 0.9117 - val_loss: 0.5596 - val_accuracy: 0.7972\n",
            "760/760 [==============================] - 17s 22ms/step - loss: 0.1192 - accuracy: 0.9681\n",
            "Training Accuracy: 0.9681\n",
            "190/190 [==============================] - 4s 23ms/step - loss: 0.5596 - accuracy: 0.7972\n",
            "Testing Accuracy:  0.7972\n"
          ]
        }
      ],
      "source": [
        "#Simple DNN Model feed forward\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length))\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(256, activation='relu'))\n",
        "model2.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model2.fit(X_train_final, y_train_final, epochs=2, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "loss, accuracy = model2.evaluate(X_train_final, y_train_final, verbose=True)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss_val, accuracy_val = model2.evaluate(X_val, y_val, verbose=True)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model CNN\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "modelCNN = Sequential()\n",
        "modelCNN.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length))\n",
        "modelCNN.add(GlobalMaxPooling1D())\n",
        "modelCNN.add(Dense(256, activation='relu'))\n",
        "modelCNN.add(Dropout(0.5))  # Adding dropout layer with 50% dropout rate\n",
        "modelCNN.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "modelCNN.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "modelCNN.fit(X_train_final, y_train_final, epochs=2, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = modelCNN.evaluate(X_val, y_val, verbose=True)\n",
        "print(f'Validation accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "id": "5elgAtTPK_BQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a53119a-38cb-4e08-840a-61822ebed77b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "760/760 [==============================] - 32s 41ms/step - loss: 0.5344 - accuracy: 0.7938 - val_loss: 0.4896 - val_accuracy: 0.8171\n",
            "Epoch 2/2\n",
            "760/760 [==============================] - 31s 41ms/step - loss: 0.3333 - accuracy: 0.8902 - val_loss: 0.5043 - val_accuracy: 0.8133\n",
            "190/190 [==============================] - 0s 2ms/step - loss: 0.5043 - accuracy: 0.8133\n",
            "Validation accuracy: 81.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNGBvhHVMwgt"
      },
      "outputs": [],
      "source": [
        "from keras import layers\n",
        "#simple rnn\n",
        "model_rnn_arabic = Sequential()\n",
        "model_rnn_arabic.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=max_length))\n",
        "model_rnn_arabic.add(layers.SimpleRNN(8, return_sequences=True, activation='tanh'))\n",
        "model_rnn_arabic.add(layers.SimpleRNN(4, activation='tanh'))\n",
        "model_rnn_arabic.add(Dense(units=3, activation='softmax'))\n",
        "\n",
        "model_rnn_arabic.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train the model\n",
        "model_rnn_arabic.fit(X_train_final, y_train_final, epochs=4, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "loss_rnn_arabic, accuracy_rnn_arabic = model_rnn_arabic.evaluate(X_val, y_val, verbose=True)\n",
        "print(\"Validation Accuracy: {:.4f}\".format(accuracy_rnn_arabic))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2V5ag2r9JSUe",
        "outputId": "7bf12601-5ec7-4de2-aefb-c38663373f03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "760/760 [==============================] - 13s 16ms/step - loss: 0.7557 - accuracy: 0.6496 - val_loss: 0.5961 - val_accuracy: 0.7807\n",
            "Epoch 2/10\n",
            "760/760 [==============================] - 10s 13ms/step - loss: 0.4994 - accuracy: 0.8226 - val_loss: 0.5581 - val_accuracy: 0.7886\n",
            "Epoch 3/10\n",
            "760/760 [==============================] - 10s 13ms/step - loss: 0.4245 - accuracy: 0.8552 - val_loss: 0.5311 - val_accuracy: 0.8076\n",
            "Epoch 4/10\n",
            "760/760 [==============================] - 10s 13ms/step - loss: 0.3808 - accuracy: 0.8725 - val_loss: 0.5332 - val_accuracy: 0.8142\n",
            "Epoch 5/10\n",
            "760/760 [==============================] - 10s 13ms/step - loss: 0.3454 - accuracy: 0.8859 - val_loss: 0.5636 - val_accuracy: 0.8067\n",
            "Epoch 6/10\n",
            "760/760 [==============================] - 10s 14ms/step - loss: 0.3088 - accuracy: 0.8964 - val_loss: 0.5589 - val_accuracy: 0.8170\n",
            "Epoch 7/10\n",
            "760/760 [==============================] - 10s 14ms/step - loss: 0.2847 - accuracy: 0.9059 - val_loss: 0.6131 - val_accuracy: 0.8132\n",
            "Epoch 8/10\n",
            "760/760 [==============================] - 10s 13ms/step - loss: 0.2531 - accuracy: 0.9144 - val_loss: 0.6252 - val_accuracy: 0.8115\n",
            "Epoch 9/10\n",
            "760/760 [==============================] - 10s 13ms/step - loss: 0.2266 - accuracy: 0.9248 - val_loss: 0.7462 - val_accuracy: 0.7965\n",
            "Epoch 10/10\n",
            "760/760 [==============================] - 10s 13ms/step - loss: 0.2056 - accuracy: 0.9311 - val_loss: 0.7024 - val_accuracy: 0.8077\n",
            "190/190 [==============================] - 0s 2ms/step - loss: 0.5589 - accuracy: 0.8170\n",
            "Validation accuracy: 81.70%\n"
          ]
        }
      ],
      "source": [
        "#feed forward neural network for text calssification cutomized\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "model3 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, 64, input_length=max_length),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(32, activation='tanh'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(32, activation='tanh'),\n",
        "    # tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')  # 3 output classes: positive, negative, neutral\n",
        "])\n",
        "\n",
        "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=4, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "model3.fit(X_train_final, y_train_final, epochs=num_epochs, validation_data=(X_val, y_val),callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model3.evaluate(X_val, y_val, verbose=True)\n",
        "print(f'Validation accuracy: {accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#another cnn\n",
        "modeltest = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, 32, input_length=max_length),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(24, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')  # 3 output classes: positive, negative, neutral\n",
        "])\n",
        "\n",
        "modeltest.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "modeltest.fit(X_train_final, y_train_final, epochs=num_epochs, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = modeltest.evaluate(X_val, y_val, verbose=True)\n",
        "print(f'Validation accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCTzKFa0C0Yf",
        "outputId": "2bc5e3f5-d657-424a-c97f-b215e1e2901c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "760/760 [==============================] - 9s 11ms/step - loss: 0.8313 - accuracy: 0.5876 - val_loss: 0.8144 - val_accuracy: 0.5916\n",
            "Epoch 2/10\n",
            "760/760 [==============================] - 7s 9ms/step - loss: 0.7347 - accuracy: 0.6634 - val_loss: 0.6721 - val_accuracy: 0.7164\n",
            "Epoch 3/10\n",
            "760/760 [==============================] - 6s 8ms/step - loss: 0.5859 - accuracy: 0.7732 - val_loss: 0.5874 - val_accuracy: 0.7913\n",
            "Epoch 4/10\n",
            "760/760 [==============================] - 5s 7ms/step - loss: 0.5000 - accuracy: 0.8222 - val_loss: 0.5402 - val_accuracy: 0.8000\n",
            "Epoch 5/10\n",
            "760/760 [==============================] - 5s 7ms/step - loss: 0.4518 - accuracy: 0.8451 - val_loss: 0.5249 - val_accuracy: 0.8036\n",
            "Epoch 6/10\n",
            "760/760 [==============================] - 5s 7ms/step - loss: 0.4226 - accuracy: 0.8575 - val_loss: 0.5150 - val_accuracy: 0.8216\n",
            "Epoch 7/10\n",
            "760/760 [==============================] - 5s 7ms/step - loss: 0.3980 - accuracy: 0.8674 - val_loss: 0.5102 - val_accuracy: 0.8201\n",
            "Epoch 8/10\n",
            "760/760 [==============================] - 8s 10ms/step - loss: 0.3795 - accuracy: 0.8729 - val_loss: 0.5130 - val_accuracy: 0.8166\n",
            "Epoch 9/10\n",
            "760/760 [==============================] - 9s 12ms/step - loss: 0.3624 - accuracy: 0.8792 - val_loss: 0.5116 - val_accuracy: 0.8221\n",
            "Epoch 10/10\n",
            "760/760 [==============================] - 8s 10ms/step - loss: 0.3457 - accuracy: 0.8855 - val_loss: 0.5131 - val_accuracy: 0.8212\n",
            "190/190 [==============================] - 0s 1ms/step - loss: 0.5131 - accuracy: 0.8212\n",
            "Validation accuracy: 82.12%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2GmoOQtjifr",
        "outputId": "83f13ebe-f78e-4f06-96b3-5738076c9dd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "760/760 [==============================] - 485s 634ms/step - loss: 0.8729 - accuracy: 0.5728 - val_loss: 0.8398 - val_accuracy: 0.5868\n",
            "Epoch 2/5\n",
            "760/760 [==============================] - 480s 632ms/step - loss: 0.6946 - accuracy: 0.6957 - val_loss: 0.5342 - val_accuracy: 0.8038\n",
            "Epoch 3/5\n",
            "760/760 [==============================] - 479s 629ms/step - loss: 0.4576 - accuracy: 0.8393 - val_loss: 0.5414 - val_accuracy: 0.8148\n",
            "Epoch 4/5\n",
            "760/760 [==============================] - 481s 633ms/step - loss: 0.3709 - accuracy: 0.8775 - val_loss: 0.5879 - val_accuracy: 0.8054\n",
            "Epoch 5/5\n",
            "760/760 [==============================] - 489s 644ms/step - loss: 0.3171 - accuracy: 0.8979 - val_loss: 0.6313 - val_accuracy: 0.8086\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 327)]             0         \n",
            "                                                                 \n",
            " token_and_position_embeddi  (None, 327, 32)           992768    \n",
            " ng_2 (TokenAndPositionEmbe                                      \n",
            " dding)                                                          \n",
            "                                                                 \n",
            " transformer_block_2 (Trans  (None, 327, 32)           19040     \n",
            " formerBlock)                                                    \n",
            "                                                                 \n",
            " global_average_pooling1d_2  (None, 32)                0         \n",
            "  (GlobalAveragePooling1D)                                       \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 15)                495       \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 15)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 3)                 48        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1012351 (3.86 MB)\n",
            "Trainable params: 1012351 (3.86 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "190/190 [==============================] - 52s 273ms/step - loss: 0.6313 - accuracy: 0.8086\n",
            "Testing Accuracy:  0.8086\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.5):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class PositionalEncoding(layers.Layer):\n",
        "    def __init__(self, max_len, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.position = tf.range(max_len, dtype=tf.float32)[:, tf.newaxis]\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def call(self, inputs):\n",
        "        i = tf.range(0, self.d_model, 2, dtype=tf.float32)\n",
        "        den = 1 / tf.pow(tf.cast(10000, tf.float32), i / tf.cast(self.d_model, tf.float32))\n",
        "        sines = tf.math.sin(self.position * den)\n",
        "        cosines = tf.math.cos(self.position * den)\n",
        "\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "\n",
        "        return tf.cast(pos_encoding+inputs, dtype=tf.float32)\n",
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = PositionalEncoding(maxlen, embed_dim)\n",
        "        self.max_len = maxlen\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.token_emb(x)\n",
        "        positions = self.pos_emb(x)\n",
        "\n",
        "        return tf.cast(positions, dtype=tf.float32)\n",
        "\n",
        "x_train = keras.utils.pad_sequences(X_train_final, maxlen=max_length)\n",
        "x_val = keras.utils.pad_sequences(X_val, maxlen=max_length)\n",
        "\n",
        "# print(x_train)\n",
        "# print(x_val)\n",
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 4  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = layers.Input(shape=(max_length,))\n",
        "embedding_layer = TokenAndPositionEmbedding(max_length, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "x = layers.Dense(15, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "outputs = layers.Dense(3, activation=\"softmax\")(x)\n",
        "\n",
        "model3 = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model3.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model3.fit(\n",
        "    x_train, y_train_final, epochs=5, validation_data=(X_val, y_val)\n",
        ")\n",
        "model3.summary()\n",
        "\n",
        "# loss, accuracy = model3.evaluate(X_train_final, y_train_final, verbose=True)\n",
        "# print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss_val, accuracy_val = model3.evaluate(X_val, y_val, verbose=True)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCM3xV7q1tYX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c865c3ea-dfc4-4f16-d82f-78a9fdc21ab3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      اهنء علي خدمه العملاء المحادثه المباشره not قص...\n",
            "1      ممتاز جدا اتم ان تكون المسابق والجواءز طلب الس...\n",
            "2              محملته يقول تم ايقاف حطيت عشان تسو الخطاء\n",
            "3                                                شغل طيب\n",
            "4                                                 ماجربت\n",
            "                             ...                        \n",
            "995                                                يستهل\n",
            "996                               خدمه سيءه بكل المعايير\n",
            "997                                        لءي٠٣٣٢لء٣٤٣س\n",
            "998                               تطبيق صادق خصم الكوبون\n",
            "999    تاخير الموظف مءهله للاسف انا استخدم طلب not سن...\n",
            "Name: review_description, Length: 1000, dtype: object\n"
          ]
        }
      ],
      "source": [
        "testing_data= pd.read_csv(\"/content/test _no_label.csv\")\n",
        "X_test= testing_data.review_description.apply(preprocessing)#applies the clean_text function to each element in the 'review_description' column of the DataFrame\n",
        "\n",
        "print(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzh9zmzdj0RQ"
      },
      "outputs": [],
      "source": [
        "j=0\n",
        "for text in X_test:\n",
        "    X_test[j]=text.strip()\n",
        "    j=j+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wrnf6Iup9gH6"
      },
      "outputs": [],
      "source": [
        "model_input=tokenizer.texts_to_sequences(X_test)\n",
        "model_input=pad_sequences(model_input, padding='post', maxlen=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSnqTm9H3TGg"
      },
      "outputs": [],
      "source": [
        "pred = modeltest.predict(model_input, verbose=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADAovCGeTXM0"
      },
      "outputs": [],
      "source": [
        "predicted_labels = np.argmax(pred, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hD1Gw_ca--C5"
      },
      "outputs": [],
      "source": [
        "predicted_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PafuwUKiESb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79a8c467-cd31-4bb1-e1de-eeb158801cb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to predictions.csv\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "sentiment_labels = [1,-1,0] #positive negative neutral\n",
        "id=1\n",
        "with open('predictions.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['ID', 'rating'])\n",
        "    for  label in predicted_labels:\n",
        "        sentiment = sentiment_labels[label]\n",
        "        writer.writerow([id, sentiment])\n",
        "        id+=1\n",
        "\n",
        "print(\"Predictions saved to predictions.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}